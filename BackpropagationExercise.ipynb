{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A notebook to exemplify backpropagation using pytorch\n",
    "\n",
    "The following notebook illustrates two ways of doing backpropagation:\n",
    "\n",
    "* by hand - using the gradients and chain rules for a custom made loss function\n",
    "* automatic - using a simple NN model in pytorch\n",
    "\n",
    "Made in the context of the course \"Pythorch for Deep Learning\" \n",
    "\n",
    "Author: P. Silva (29/12/2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "A simple network with 1 input, 1 hidded and 1 output layer, each with two perceptrons making use of a logistic activiation function.\n",
    "The values and architecture are based on [Matt Mazur's blog](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/). The only difference is that we allow the individual biases of the perceptrons in the same layer to be updated separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_weights=[[0.15,0.20,0.25,0.30],[0.40,0.45,0.50,0.55]]\n",
    "layer_biases=[[0.35,0.35],[0.60,0.60]]\n",
    "inputs=[0.05,0.10]\n",
    "exp_outputs=[0.01,0.99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: doing back-propagation by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass\n",
    "\n",
    "We compute the values expected as output of the hidden and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer: [[0.59327   ]\n",
      " [0.59688437]]\n",
      "Output layer: [[0.75136507]\n",
      " [0.7729285 ]]\n"
     ]
    }
   ],
   "source": [
    "#input layer\n",
    "inputT = torch.Tensor(inputs).reshape(-1,1)\n",
    "\n",
    "def getLayerAsTensor(inputT,weights,bias):\n",
    "   \n",
    "    if isinstance(bias,torch.Tensor):\n",
    "        biasT = bias\n",
    "    else:\n",
    "        biasT = torch.tensor(np.array(bias).reshape(2,1), requires_grad=True, dtype=torch.float32)\n",
    "    \n",
    "    if isinstance(weights,torch.Tensor):\n",
    "        weightsT = weights\n",
    "    else:\n",
    "        weightsT = torch.tensor(np.array(weights).reshape(2,2), requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "    activation = torch.nn.Sigmoid()\n",
    "    layerT     = activation( torch.add(weightsT.mm(inputT),biasT) )\n",
    "\n",
    "    return layerT,weightsT,biasT\n",
    "    \n",
    "\n",
    "#hidden layer definition\n",
    "hiddenT, hiddenWeightsT, hiddenBiasT = getLayerAsTensor(inputT,  layer_weights[0], layer_biases[0])\n",
    "outT,    outWeightsT,    outBiasT    = getLayerAsTensor(hiddenT, layer_weights[1], layer_biases[1])\n",
    "\n",
    "print('Hidden layer:', hiddenT.detach().numpy())\n",
    "print('Output layer:', outT.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error: 0.2983711\n"
     ]
    }
   ],
   "source": [
    "#error computation\n",
    "def getMSE(outT,exp_outputs):\n",
    "    expOutT = torch.tensor(exp_outputs, dtype=torch.float32).reshape(-1,1)\n",
    "    return ( 0.5*( outT - expOutT )**2 ).sum()\n",
    "\n",
    "errorT=getMSE(outT,exp_outputs)\n",
    "print('Total error:',errorT.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass\n",
    "\n",
    "We now apply the chain rule to perform back propagation and update the weights so that the final error is minimized.\n",
    "The backward pass with gradient descent is given by the formula\n",
    "\n",
    "$\\vec{\\theta}_{n+1}=\\vec{\\theta}_{n}-\\eta \\vec{\\nabla} C(\\vec{\\theta}_n)$\n",
    "\n",
    "with the following quantities defined\n",
    "\n",
    "* $\\vec{\\theta}=(\\vec{w},\\vec{b})$ a vector of weights and biases\n",
    "* $\\eta$ the learning rate\n",
    "* C the cost function in this case the sum of the mean square errors of the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the derivatives of the cost function\n",
    "errorT.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Udpdated hidden layer\n",
      "\t weights [[0.14978072 0.19956143]\n",
      " [0.24975115 0.2995023 ]]\n",
      "\t bias [[0.3456143 ]\n",
      " [0.34502286]]\n",
      "Udpdated output layer\n",
      "\t weights [[0.3589165  0.40866616]\n",
      " [0.5113013  0.56137013]]\n",
      "\t bias [[0.53075075]\n",
      " [0.61904913]]\n"
     ]
    }
   ],
   "source": [
    "#gradient descent algorithm\n",
    "def doGradientDescentBackProp(t,lr=0.5):\n",
    "    return t.add(t.grad,alpha=-lr)\n",
    "    \n",
    "newHiddenWeightsT=doGradientDescentBackProp(hiddenWeightsT)\n",
    "newHiddenBiasT=doGradientDescentBackProp(hiddenBiasT)\n",
    "newOutWeightsT=doGradientDescentBackProp(outWeightsT)\n",
    "newOutBiasT=doGradientDescentBackProp(outBiasT)\n",
    "\n",
    "print('Udpdated hidden layer')\n",
    "print('\\t weights',newHiddenWeightsT.detach().numpy())\n",
    "print('\\t bias',newHiddenBiasT.detach().numpy())\n",
    "\n",
    "print('Udpdated output layer')\n",
    "print('\\t weights',newOutWeightsT.detach().numpy())\n",
    "print('\\t bias',newOutBiasT.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error (no updated biases): 0.2910278\n",
      "Total error (updated biases): 0.28047147\n"
     ]
    }
   ],
   "source": [
    "#build the updated network and get its error\n",
    "newHiddenT, _, _ = getLayerAsTensor(inputT,     newHiddenWeightsT, hiddenBiasT)\n",
    "newOutT,    _, _ = getLayerAsTensor(newHiddenT, newOutWeightsT,    outBiasT)\n",
    "newErrorT        = getMSE(newOutT,exp_outputs)\n",
    "print('Total error (no updated biases):',newErrorT.detach().numpy())\n",
    "\n",
    "newHiddenT, _, _ = getLayerAsTensor(inputT,     newHiddenWeightsT, newHiddenBiasT)\n",
    "newOutT,    _, _ = getLayerAsTensor(newHiddenT, newOutWeightsT,    newOutBiasT)\n",
    "newErrorT        = getMSE(newOutT,exp_outputs)\n",
    "print('Total error (updated biases):',newErrorT.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: implementation using a simple ANN model\n",
    "\n",
    "We repeat the above exercise but using a ANN class derived from torch.nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_0.weight Parameter containing:\n",
      "tensor([[0.1500, 0.2000],\n",
      "        [0.2500, 0.3000]], requires_grad=True)\n",
      "layer_0.bias Parameter containing:\n",
      "tensor([0.3500, 0.3500], requires_grad=True)\n",
      "layer_1.weight Parameter containing:\n",
      "tensor([[0.4000, 0.4500],\n",
      "        [0.5000, 0.5500]], requires_grad=True)\n",
      "layer_1.bias Parameter containing:\n",
      "tensor([0.6000, 0.6000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    \"\"\"this model implements a dummy 2 layer ANN to explore the backpropagation algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self,layer_weights,layer_biases):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nlayers=len(layer_weights)\n",
    "        for i in range(self.nlayers):       \n",
    "            setattr(self,'layer_%d'%i, nn.Linear(2,2) )\n",
    "            layer=getattr(self,'layer_%d'%i)\n",
    "            \n",
    "            layer.bias.data = torch.tensor( np.array(layer_biases[i]), dtype=torch.float32 )\n",
    "            layer.weight.data = torch.tensor(np.array(layer_weights[i]).reshape(2,2), dtype=torch.float32)\n",
    "                        \n",
    "    \n",
    "    def forward(self,x,debug=True):\n",
    "        for i in range(self.nlayers):\n",
    "            x = F.sigmoid( getattr(self,'layer_%d'%i)(x) )\n",
    "            if debug:\n",
    "                print('Layer {} yields {}'.format(i,str(x)))\n",
    "        return x\n",
    "    \n",
    "    def print(self):\n",
    "        for name,param in self.named_parameters():\n",
    "            print(name,param)\n",
    "    \n",
    "model=Model(layer_weights,layer_biases)\n",
    "model.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass \n",
    "\n",
    "The forward pass is computed for the given inputs and the loss is defined using the standard MSELoss class from pytorch.\n",
    "the results are fully equivalent to the ones obtained previously by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 yields tensor([0.5933, 0.5969], grad_fn=<SigmoidBackward>)\n",
      "Layer 1 yields tensor([0.7514, 0.7729], grad_fn=<SigmoidBackward>)\n",
      "Error: tensor(0.2984, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pmvcfs/.conda/envs/pytorchenv/lib/python3.7/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "criterion=nn.MSELoss()\n",
    "\n",
    "x=torch.tensor(inputs)\n",
    "yexp=torch.tensor(exp_outputs)\n",
    "\n",
    "#make the forward pass and compute the loss with MSE\n",
    "y=model.forward(x)\n",
    "loss=criterion(yexp,y)\n",
    "print('Error:',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass\n",
    "\n",
    "We use the stochastic gradient descent from pytorch with only the learning rate set to the same value as previously.\n",
    "This way the result is expected to yield the same as a simple gradient descent algorithm.\n",
    "The results are equivalent to the ones previously obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_0.weight Parameter containing:\n",
      "tensor([[0.1498, 0.1996],\n",
      "        [0.2498, 0.2995]], requires_grad=True)\n",
      "layer_0.bias Parameter containing:\n",
      "tensor([0.3456, 0.3450], requires_grad=True)\n",
      "layer_1.weight Parameter containing:\n",
      "tensor([[0.3589, 0.4087],\n",
      "        [0.5113, 0.5614]], requires_grad=True)\n",
      "layer_1.bias Parameter containing:\n",
      "tensor([0.5308, 0.6190], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#make the backward propagation with gradient descent\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.5)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "model.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final forward pass\n",
    "\n",
    "The forward pass with the updated weights and biases has an error which is similar to the one obtained by hand previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 yields tensor([0.5922, 0.5957], grad_fn=<SigmoidBackward>)\n",
      "Layer 1 yields tensor([0.7284, 0.7784], grad_fn=<SigmoidBackward>)\n",
      "Error: tensor(0.2805, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_new=model.forward(x)\n",
    "loss_new=criterion(yexp,y_new)\n",
    "print('Error:',loss_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
